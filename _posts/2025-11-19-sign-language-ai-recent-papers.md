---
layout: post
title: "Sign Language & AI – Recent Papers (November 19, 2025)"
date: 2025-11-19 00:26:05 +0000
categories: [sign-language, ai]
tags: [sign-language, AI, ASL, research]
---

This digest automatically gathers recent arXiv papers related to sign language, sign language translation/recognition, and sign language & AI.

## 1. A Comparative Analysis of Recurrent and Attention Architectures for Isolated Sign Language Recognition

- **Authors:** Nigar Alishzade, Gulchin Abdullayeva
- **Published:** 2025-11-17
- **arXiv:** [http://arxiv.org/abs/2511.13126v1](http://arxiv.org/abs/2511.13126v1)
- **Tags:** ASL, neural-network, recognition, transformer

**Summary**

This study presents a systematic comparative analysis of recurrent and attention-based neural architectures for isolated sign language recognition. We implement and evaluate two representative models-ConvLSTM and Vanilla Transformer-on the Azerbaijani Sign Language Dataset (AzSLD) and the Word-Level American Sign Language (WLASL) dataset. Our results demonstrate that the attention-based Vanilla Transformer consistently outperforms the recurrent ConvLSTM in both Top-1 and Top-5 accuracy across…

**Why it matters**

This paper is relevant because it advances sign language recognition.

---


## 2. RoCoISLR: A Romanian Corpus for Isolated Sign Language Recognition

- **Authors:** Cătălin-Alexandru Rîpanu, Andrei-Theodor Hotnog, Giulia-Stefania Imbrea, Dumitru-Clementin Cercel
- **Published:** 2025-11-16
- **arXiv:** [http://arxiv.org/abs/2511.12767v1](http://arxiv.org/abs/2511.12767v1)
- **Tags:** ASL, pose-estimation, recognition, transformer

**Summary**

Automatic sign language recognition plays a crucial role in bridging the communication gap between deaf communities and hearing individuals; however, most available datasets focus on American Sign Language. For Romanian Isolated Sign Language Recognition (RoISLR), no large-scale, standardized dataset exists, which limits research progress. In this work, we introduce a new corpus for RoISLR, named RoCoISLR, comprising over 9,000 video samples that span nearly 6,000 standardized glosses from…

**Why it matters**

This paper is relevant because it advances sign language recognition.

---


## 3. BdSL-SPOTER: A Transformer-Based Framework for Bengali Sign Language Recognition with Cultural Adaptation

- **Authors:** Sayad Ibna Azad, Md. Atiqur Rahman
- **Published:** 2025-11-15
- **arXiv:** [http://arxiv.org/abs/2511.12103v1](http://arxiv.org/abs/2511.12103v1)
- **Tags:** pose-estimation, recognition, transformer

**Summary**

We introduce BdSL-SPOTER, a pose-based transformer framework for accurate and efficient recognition of Bengali Sign Language (BdSL). BdSL-SPOTER extends the SPOTER paradigm with cultural specific preprocessing and a compact four-layer transformer encoder featuring optimized learnable positional encodings, while employing curriculum learning to enhance generalization on limited data and accelerate convergence. On the BdSLW60 benchmark, it achieves 97.92% Top-1 validation accuracy, representing a…

**Why it matters**

This paper is relevant because it advances sign language recognition.

---


## 4. Large Sign Language Models: Toward 3D American Sign Language Translation

- **Authors:** Sen Zhang, Xiaoxiao He, Di Liu, Zhaoyang Xia, Mingyu Zhao, Chaowei Tan, Vivian Li, Bo Liu, Dimitris N. Metaxas, Mubbasir Kapadia
- **Published:** 2025-11-11
- **arXiv:** [http://arxiv.org/abs/2511.08535v1](http://arxiv.org/abs/2511.08535v1)
- **Tags:** ASL, recognition, translation

**Summary**

We present Large Sign Language Models (LSLM), a novel framework for translating 3D American Sign Language (ASL) by leveraging Large Language Models (LLMs) as the backbone, which can benefit hearing-impaired individuals' virtual communication. Unlike existing sign language recognition methods that rely on 2D video, our approach directly utilizes 3D sign language data to capture rich spatial, gestural, and depth information in 3D scenes. This enables more accurate and resilient translation,…

**Why it matters**

This paper is relevant because it tackles sign language translation.

---


## 5. Introducing A Bangla Sentence - Gloss Pair Dataset for Bangla Sign Language Translation and Research

- **Authors:** Neelavro Saha, Rafi Shahriyar, Nafis Ashraf Roudra, Saadman Sakib, Annajiat Alim Rasel
- **Published:** 2025-11-11
- **arXiv:** [http://arxiv.org/abs/2511.08507v1](http://arxiv.org/abs/2511.08507v1)
- **Tags:** transformer, translation

**Summary**

Bangla Sign Language (BdSL) translation represents a low-resource NLP task due to the lack of large-scale datasets that address sentence-level translation. Correspondingly, existing research in this field has been limited to word and alphabet level detection. In this work, we introduce Bangla-SGP, a novel parallel dataset consisting of 1,000 human-annotated sentence-gloss pairs which was augmented with around 3,000 synthetically generated pairs using syntactic and morphological rules through a…

**Why it matters**

This paper is relevant because it tackles sign language translation.

---


## 6. Sign language recognition from skeletal data using graph and recurrent neural networks

- **Authors:** B. Mederos, J. Mejía, A. Medina-Reyes, Y. Espinosa-Almeyda, J. D. Díaz-Roman, I. Rodríguez-Mederos, M. Mejía-Carreon, F. Gonzalez-Lopez
- **Published:** 2025-11-08
- **arXiv:** [http://arxiv.org/abs/2511.05772v1](http://arxiv.org/abs/2511.05772v1)
- **Tags:** neural-network, pose-estimation, recognition

**Summary**

This work presents an approach for recognizing isolated sign language gestures using skeleton-based pose data extracted from video sequences. A Graph-GRU temporal network is proposed to model both spatial and temporal dependencies between frames, enabling accurate classification. The model is trained and evaluated on the AUTSL (Ankara university Turkish sign language) dataset, achieving high accuracy. Experimental results demonstrate the effectiveness of integrating graph-based spatial…

**Why it matters**

This paper is relevant because it advances sign language recognition.

---


## 7. Smart-Hiring: An Explainable end-to-end Pipeline for CV Information Extraction and Job Matching

- **Authors:** Kenza Khelkhal, Dihia Lanasri
- **Published:** 2025-11-04
- **arXiv:** [http://arxiv.org/abs/2511.02537v1](http://arxiv.org/abs/2511.02537v1)
- **Tags:** pose-estimation, recognition

**Summary**

Hiring processes often involve the manual screening of hundreds of resumes for each job, a task that is time and effort consuming, error-prone, and subject to human bias. This paper presents Smart-Hiring, an end-to-end Natural Language Processing (NLP) pipeline de- signed to automatically extract structured information from unstructured resumes and to semantically match candidates with job descriptions. The proposed system combines document parsing, named-entity recognition, and contextual text…

**Why it matters**

This paper is relevant because it advances sign language recognition.

---


## 8. POSESTITCH-SLT: Linguistically Inspired Pose-Stitching for End-to-End Sign Language Translation

- **Authors:** Abhinav Joshi, Vaibhav Sharma, Sanjeet Singh, Ashutosh Modi
- **Published:** 2025-10-31
- **arXiv:** [http://arxiv.org/abs/2511.00270v1](http://arxiv.org/abs/2511.00270v1)
- **Tags:** neural-network, pose-estimation, transformer, translation

**Summary**

Sign language translation remains a challenging task due to the scarcity of large-scale, sentence-aligned datasets. Prior arts have focused on various feature extraction and architectural changes to support neural machine translation for sign languages. We propose POSESTITCH-SLT, a novel pre-training scheme that is inspired by linguistic-templates-based sentence generation technique. With translation comparison on two sign language datasets, How2Sign and iSign, we show that a simple…

**Why it matters**

This paper is relevant because it tackles sign language translation.

---


## 9. GLYPH-SR: Can We Achieve Both High-Quality Image Super-Resolution and High-Fidelity Text Recovery via VLM-guided Latent Diffusion Model?

- **Authors:** Mingyu Sung, Seungjae Ham, Kangwoo Kim, Yeokyoung Yoon, Sangseok Yun, Il-Min Kim, Jae-Mo Kang
- **Published:** 2025-10-30
- **arXiv:** [http://arxiv.org/abs/2510.26339v1](http://arxiv.org/abs/2510.26339v1)
- **Tags:** recognition

**Summary**

Image super-resolution(SR) is fundamental to many vision system-from surveillance and autonomy to document analysis and retail analytics-because recovering high-frequency details, especially scene-text, enables reliable downstream perception. Scene-text, i.e., text embedded in natural images such as signs, product labels, and storefronts, often carries the most actionable information; when characters are blurred or hallucinated, optical character recognition(OCR) and subsequent decisions fail…

**Why it matters**

This paper is relevant because it advances sign language recognition.

---


## 10. A Critical Study of Automatic Evaluation in Sign Language Translation

- **Authors:** Shakib Yazdani, Yasser Hamidullah, Cristina España-Bonet, Eleftherios Avramidis, Josef van Genabith
- **Published:** 2025-10-29
- **arXiv:** [http://arxiv.org/abs/2510.25434v2](http://arxiv.org/abs/2510.25434v2)
- **Tags:** translation

**Summary**

Automatic evaluation metrics are crucial for advancing sign language translation (SLT). Current SLT evaluation metrics, such as BLEU and ROUGE, are only text-based, and it remains unclear to what extent text-based metrics can reliably capture the quality of SLT outputs. To address this gap, we investigate the limitations of text-based SLT evaluation metrics by analyzing six metrics, including BLEU, chrF, and ROUGE, as well as BLEURT on the one hand, and large language model (LLM)-based…

**Why it matters**

This paper is relevant because it tackles sign language translation.

---


## 11. Seeing, Signing, and Saying: A Vision-Language Model-Assisted Pipeline for Sign Language Data Acquisition and Curation from Social Media

- **Authors:** Shakib Yazdani, Yasser Hamidullah, Cristina España-Bonet, Josef van Genabith
- **Published:** 2025-10-29
- **arXiv:** [http://arxiv.org/abs/2510.25413v1](http://arxiv.org/abs/2510.25413v1)
- **Tags:** ASL, pose-estimation, recognition, translation

**Summary**

Most existing sign language translation (SLT) datasets are limited in scale, lack multilingual coverage, and are costly to curate due to their reliance on expert annotation and controlled recording setup. Recently, Vision Language Models (VLMs) have demonstrated strong capabilities as evaluators and real-time assistants. Despite these advancements, their potential remains untapped in the context of sign language dataset acquisition. To bridge this gap, we introduce the first automated…

**Why it matters**

This paper is relevant because it tackles sign language translation.

---


## 12. Proper Body Landmark Subset Enables More Accurate and 5X Faster Recognition of Isolated Signs in LIBRAS

- **Authors:** Daniele L. V. dos Santos, Thiago B. Pereira, Carlos Eduardo G. R. Alves, Richard J. M. G. Tello, Francisco de A. Boldt, Thiago M. Paixão
- **Published:** 2025-10-28
- **arXiv:** [http://arxiv.org/abs/2510.24887v1](http://arxiv.org/abs/2510.24887v1)
- **Tags:** pose-estimation, recognition

**Summary**

This paper investigates the feasibility of using lightweight body landmark detection for the recognition of isolated signs in Brazilian Sign Language (LIBRAS). Although the skeleton-based approach by Alves et al. (2024) enabled substantial improvements in recognition performance, the use of OpenPose for landmark extraction hindered time performance. In a preliminary investigation, we observed that simply replacing OpenPose with the lightweight MediaPipe, while improving processing speed,…

**Why it matters**

This paper is relevant because it advances sign language recognition.

---


## 13. Reconnaissance Automatique des Langues des Signes : Une Approche Hybridée CNN-LSTM Basée sur Mediapipe

- **Authors:** Fraisse Sacré Takouchouang, Ho Tuong Vinh
- **Published:** 2025-10-24
- **arXiv:** [http://arxiv.org/abs/2510.22011v1](http://arxiv.org/abs/2510.22011v1)
- **Tags:** pose-estimation, recognition, translation

**Summary**

Sign languages play a crucial role in the communication of deaf communities, but they are often marginalized, limiting access to essential services such as healthcare and education. This study proposes an automatic sign language recognition system based on a hybrid CNN-LSTM architecture, using Mediapipe for gesture keypoint extraction. Developed with Python, TensorFlow and Streamlit, the system provides real-time gesture translation. The results show an average accuracy of 92\%, with very good…

**Why it matters**

This paper is relevant because it tackles sign language translation.

---


## 14. SIGN: Schema-Induced Games for Naming

- **Authors:** Ryan Zhang, Herbert Woisetscläger
- **Published:** 2025-10-22
- **arXiv:** [http://arxiv.org/abs/2510.21855v1](http://arxiv.org/abs/2510.21855v1)
- **Tags:** sign-language-ai

**Summary**

Real-world AI systems are tackling increasingly complex problems, often through interactions among large language model (LLM) agents. When these agents develop inconsistent conventions, coordination can break down. Applications such as collaborative coding and distributed planning therefore require reliable, consistent communication, and scalability is a central concern as systems grow. We introduce Schema-Induced Games for Naming (SIGN), a naming game that examines how lightweight structure…

**Why it matters**

This paper explores methods that can be applied to sign language and AI.

---


## 15. Spatio-temporal Sign Language Representation and Translation

- **Authors:** Yasser Hamidullah, Josef van Genabith, Cristina España-Bonet
- **Published:** 2025-10-22
- **arXiv:** [http://arxiv.org/abs/2510.19413v1](http://arxiv.org/abs/2510.19413v1)
- **Tags:** translation

**Summary**

This paper describes the DFKI-MLT submission to the WMT-SLT 2022 sign language translation (SLT) task from Swiss German Sign Language (video) into German (text). State-of-the-art techniques for SLT use a generic seq2seq architecture with customized input embeddings. Instead of word embeddings as used in textual machine translation, SLT systems use features extracted from video frames. Standard approaches often do not benefit from temporal features. In our participation, we present a system that…

**Why it matters**

This paper is relevant because it tackles sign language translation.

---


## 16. SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision

- **Authors:** Yasser Hamidullah, Shakib Yazdani, Cennet Oguz, Josef van Genabith, Cristina España-Bonet
- **Published:** 2025-10-22
- **arXiv:** [http://arxiv.org/abs/2510.19398v1](http://arxiv.org/abs/2510.19398v1)
- **Tags:** pose-estimation, translation

**Summary**

Sign language translation (SLT) is typically trained with text in a single spoken language, which limits scalability and cross-language generalization. Earlier approaches have replaced gloss supervision with text-based sentence embeddings, but up to now, these remain tied to a specific language and modality. In contrast, here we employ language-agnostic, multimodal embeddings trained on text and speech from multiple languages to supervise SLT, enabling direct multilingual translation. To…

**Why it matters**

This paper is relevant because it tackles sign language translation.

---


## 17. Sign Language Translation with Sentence Embedding Supervision

- **Authors:** Yasser Hamidullah, Josef van Genabith, Cristina España-Bonet
- **Published:** 2025-10-22
- **arXiv:** [http://arxiv.org/abs/2510.19367v1](http://arxiv.org/abs/2510.19367v1)
- **Tags:** translation

**Summary**

State-of-the-art sign language translation (SLT) systems facilitate the learning process through gloss annotations, either in an end2end manner or by involving an intermediate step. Unfortunately, gloss labelled sign language data is usually not available at scale and, when available, gloss annotations widely differ from dataset to dataset. We present a novel approach using sentence embeddings of the target sentences at training time that take the role of glosses. The new kind of supervision…

**Why it matters**

This paper is relevant because it tackles sign language translation.

---


## 18. Grounding or Guessing? Visual Signals for Detecting Hallucinations in Sign Language Translation

- **Authors:** Yasser Hamidullah, Koel Dutta Chowdury, Yusser Al-Ghussin, Shakib Yazdani, Cennet Oguz, Josef van Genabith, Cristina España-Bonet
- **Published:** 2025-10-21
- **arXiv:** [http://arxiv.org/abs/2510.18439v1](http://arxiv.org/abs/2510.18439v1)
- **Tags:** pose-estimation, translation

**Summary**

Hallucination, where models generate fluent text unsupported by visual evidence, remains a major flaw in vision-language models and is particularly critical in sign language translation (SLT). In SLT, meaning depends on precise grounding in video, and gloss-free models are especially vulnerable because they map continuous signer movements directly into natural language without intermediate gloss supervision that serves as alignment. We argue that hallucinations arise when models rely on…

**Why it matters**

This paper is relevant because it tackles sign language translation.

---

